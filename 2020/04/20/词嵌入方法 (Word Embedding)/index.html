<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="mariaclark1123@outlook.com"><title>词嵌入方法 (Word Embedding) · Fang-da</title><meta name="description" content="自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是用来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌入（word embedding）。近年来，词嵌入已逐渐成为自然语言处理的基础知识。
 为何不采用one-hot向量
我们在自"><meta name="keywords" content="Hexo,HTML,CSS,android,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/judy.jpg" style="width:160px;"><h3 title=""><a href="/" style="font-family:medium-ui-sans-serif-text-font,-apple-system,BlinkMacSystemFont,&quot;Segoe UI&quot;,Roboto,Oxygen,Ubuntu,Cantarell,&quot;Open Sans&quot;,&quot;Helvetica Neue&quot;,sans-serif;">Fang-da</a></h3><div class="description"><p>生活一分一秒的过, 你的斗志不是燃烧一刻，而是要每分每秒的燃烧, 即使不燃烧，至少不能让它熄灭。你要的斗志从来都不在逆境中, 而在你做成一件事后这件事能给你带来的成就感以及未来, 那是能指导你每分每秒都燃烧的火药。</p></div></div></div><ul class="social-links"><li><a href="http://facebook.com/mariaclark1123"><i class="fa fa-facebook"></i></a></li><li><a href="http://github.com/mariaclark1123"><i class="fa fa-github"></i></a></li></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Theme from Fangda&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/albums">相册</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>词嵌入方法 (Word Embedding)</a></h3></div><div class="post-content"><p>自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是用来表示词的向量，也可被认为是词的特征向量或表征。<strong>把词映射为实数域向量的技术也叫词嵌入（word embedding）</strong>。近年来，词嵌入已逐渐成为自然语言处理的基础知识。</p>
<h3 id="为何不采用one-hot向量"><a class="markdownIt-Anchor" href="#为何不采用one-hot向量"></a> 为何不采用one-hot向量</h3>
<p>我们在<a href="http://fangda.me/2020/04/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%20-%20%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%20(Representation)/" target="_blank" rel="external">自然语言处理 - 文本表示 (REPRESENTATION)</a>了解到可以用one-hot向量表示词（字符为词）。回忆一下，假设词典中不同词的数量（词典大小）为 N ，每个词可以和从0到 N−1 的连续整数一一对应。这些与词对应的整数叫作词的索引。 假设一个词的索引为 i ，为了得到该词的one-hot向量表示，我们创建一个全0的长为 N 的向量，并将其第 i 位设成1。这样一来，每个词就表示成了一个长度为 N 的向量，可以直接被神经网络使用。</p>
<p>虽然one-hot词向量构造起来很容易，但通常并不是一个好选择。一个主要的原因是，<strong>one-hot词向量无法准确表达不同词之间的相似度</strong>，如我们常常使用的余弦相似度。对于向量 <img src="http://latex.codecogs.com/gif.latex?a,b\in R^{d}">，它们的余弦相似度是它们之间夹角的余弦值</p>
<center> <img src="http://latex.codecogs.com/gif.latex?\frac{a.b}{|a|*|b|} \in [-1, 1]"></center>
<p><strong>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</strong></p>
<p>word2vec工具的提出正是为了解决上面这个问题。它将每个词表示成一个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系。word2vec工具包含了两个模型，<strong>即跳字模型（skip-gram）</strong> 和<strong>连续词袋模型（continuous bag of words，CBOW）</strong>。接下来让我们分别介绍这两个模型以及它们的训练方法。</p>
<h3 id="跳字模型skip-gram"><a class="markdownIt-Anchor" href="#跳字模型skip-gram"></a> 跳字模型（skip-gram）</h3>
<p>跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即</p>
<center>P("the","man","his","son" ∣ "loves")</center>
<p>假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成</p>
<center>P("the" ∣ "loves") ⋅ P("man" ∣ "loves") ⋅ P("his" ∣ "loves") ⋅ P("son" ∣ "loves")</center>
<p>在跳字模型中，每个词被表示成两个 d 维向量，用来计算条件概率。假设这个词在词典中索引为 i ，当它为中心词时向量表示为  <img src="http://latex.codecogs.com/gif.latex?v_{i}\in R^{d}"> ，而为背景词时向量表示为  <img src="http://latex.codecogs.com/gif.latex?u_{i}\in R^{d}">。设中心词 wc 在词典中索引为 c ，背景词 wo 在词典中索引为 o ，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：</p>
<center> <img src="http://latex.codecogs.com/gif.latex?P(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)},"></center>
<p>其中词典索引集 <img src="http://latex.codecogs.com/gif.latex?\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}">。假设给定一个长度为 T 的文本序列，设时间步 t 的词为 <img src="http://latex.codecogs.com/gif.latex?w^{(t)}">。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 m 时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率</p>
<center> <img src="http://latex.codecogs.com/gif.latex?\prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)})"></center>
<p>这里小于1或大于 T 的时间步可以被忽略。</p>
<h3 id="训练跳字模型"><a class="markdownIt-Anchor" href="#训练跳字模型"></a> 训练跳字模型</h3>
<p>跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<center> <img src="http://latex.codecogs.com/gif.latex?- \sum_{t=1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \text{log}\, P(w^{(t+j)} \mid w^{(t)})"></center>
<p>如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到</p>
<center> <img src="http://latex.codecogs.com/gif.latex?\log P(w_o \mid w_c) =
\boldsymbol{u}_o^\top \boldsymbol{v}_c - \log\left(\sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)"></center>
<p>通过微分，我们可以得到上式中 vc 的梯度</p>
<center><img src="http://latex.codecogs.com/gif.latex?
\frac{\partial \text{log}\, P(w_o \mid w_c)}{\partial \boldsymbol{v}_c}
\\ = \boldsymbol{u}_o - \frac{\sum_{j \in \mathcal{V}} \exp(\boldsymbol{u}_j^\top \boldsymbol{v}_c)\boldsymbol{u}_j}{\sum_{i \in \mathcal{V}} \exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\\ = \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} \left(\frac{\text{exp}(\boldsymbol{u}_j^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\right) \boldsymbol{u}_j\\ = \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} P(w_j \mid w_c) \boldsymbol{u}_j"></center>
<p>它的计算需要词典中所有词以 <img src="http://latex.codecogs.com/gif.latex?w_c"> 为中心词的条件概率。有关其他词向量的梯度同理可得。</p>
<p>训练结束后，对于词典中的任一索引为 i 的词，我们均得到该词作为中心词和背景词的两组词向量  <img src="http://latex.codecogs.com/gif.latex?v_i">  和  <img src="http://latex.codecogs.com/gif.latex?u_i"> 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。</p>
<hr>
<p>参考文章：<br>
<a href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="external">词嵌入（word2vec）</a></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2020-04-20</span><i class="fa fa-tag"></i><a href="/tags/Machine-Learning/" title="Machine Learning" class="tag">Machine Learning </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/2020/04/20/词嵌入方法 (Word Embedding)/,Fang-da,词嵌入方法 (Word Embedding),;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2020/04/20/sklearn计算文本相似度的方法/" title="sklearn计算文本相似度的方法" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2020/04/16/利用余弦相似性比较两篇文章的相似程度/" title="利用余弦相似性比较两篇文章的相似程度" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>