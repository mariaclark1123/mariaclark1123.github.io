<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="mariaclark1123@outlook.com"><title>人工智能优化算法（一） - 梯度下降（Gradient Descent） · Fang-da</title><meta name="description" content="梯度下降法是机器学习中一种常用到的算法，是一种求解的最优化算法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。
一个梯度下降的经典例子梯度下降法的基本思想可以类比为一个下山的过程。假设一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但"><meta name="keywords" content="Hexo,HTML,CSS,android,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/judy.jpg" style="width:160px;"><h3 title><a href="/" style="font-family:medium-ui-sans-serif-text-font,-apple-system,BlinkMacSystemFont,&quot;Segoe UI&quot;,Roboto,Oxygen,Ubuntu,Cantarell,&quot;Open Sans&quot;,&quot;Helvetica Neue&quot;,sans-serif;">Fang-da</a></h3><div class="description"><p>生活一分一秒的过, 你的斗志不是燃烧一刻，而是要每分每秒的燃烧, 即使不燃烧，至少不能让它熄灭。你要的斗志从来都不在逆境中, 而在你做成一件事后这件事能给你带来的成就感以及未来, 那是能指导你每分每秒都燃烧的火药。</p></div></div></div><ul class="social-links"><li><a href="http://facebook.com/mariaclark1123"><i class="fa fa-facebook"></i></a></li><li><a href="http://github.com/mariaclark1123"><i class="fa fa-github"></i></a></li></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Theme from Fangda&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/albums">相册</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>人工智能优化算法（一） - 梯度下降（Gradient Descent）</a></h3></div><div class="post-content"><p>梯度下降法是机器学习中一种常用到的算法，是一种求解的最优化算法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。</p>
<h3 id="一个梯度下降的经典例子"><a href="#一个梯度下降的经典例子" class="headerlink" title="一个梯度下降的经典例子"></a>一个梯度下降的经典例子</h3><p>梯度下降法的基本思想可以类比为一个下山的过程。假设一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p>
<p><img src="https://github.com/mariaclark1123/mariaclark1123.github.io/blob/master/srcimage/gradient%20desecent.jpg?raw=true" alt></p>
<p>可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度是一个向量，向量有方向，我们说的梯度方向，其实就是函数变化率最大的方向，这就是梯度的几何意义。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。</p>
<p><strong>梯度的公式如下：</strong></p>
<p><img src="http://latex.codecogs.com/gif.latex?gradf(x_0,x_1,...,x_n)=(\frac{\partial}{\partial x_0},...,\frac{\partial}{\partial x_j},...,\frac{\partial}{\partial x_n})
"></p>
<p>　这里注意三点：<br>　1）梯度是一个向量，即有方向有大小；<br>　2）梯度的方向是最大方向导数的方向；<br>　3）梯度的值（这里指的是模）是最大方向导数（导数本身就是值）  
　</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><strong>梯度下降算法背后的原理：</strong><br>目标函数J(θ)关于参数θ的梯度是目标函数上升最快的方向。对于最小化优化问题，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数的下降。即给定一个与参数 θ 有关的目标函数J(θ), 求使得 J 最小的参数θ.我们把这个步长又称为学习速率η。</p>
<p><strong>参数更新公式如下：</strong></p>
<p><img src="http://latex.codecogs.com/gif.latex?\theta \leftarrow \theta - \eta \cdot\nabla_\theta J(\theta)"></p>
<p>其中∇θJ(θ)是参数的梯度，根据计算目标函数J(θ)采用数据量的不同，梯度下降算法又可以分为<strong>批量梯度下降算法（Batch Gradient Descent）</strong>，<strong>随机梯度下降算法（Stochastic GradientDescent）</strong>和<strong>小批量梯度下降算法（Mini-batch Gradient Descent）</strong>。对于批量梯度下降算法，其J(θ)是在整个训练集上计算的，如果数据集比较大，可能会面临内存不足问题，而且其收敛速度一般比较慢。随机梯度下降算法是另外一个极端，J(θ)是针对训练集中的一个训练样本计算的，又称为在线学习，即得到了一个样本，就可以执行一次参数更新。所以其收敛速度会快一些，但是有可能出现目标函数值震荡现象，因为高频率的参数更新导致了高方差。小批量梯度下降算法是折中方案，选取训练集中一个小批量样本计算J(θ)，这样可以保证训练过程更稳定，而且采用批量训练方法也可以利用矩阵计算的优势。这是目前最常用的梯度下降算法。</p>
<h3 id="一维梯度下降"><a href="#一维梯度下降" class="headerlink" title="一维梯度下降"></a>一维梯度下降</h3><p>我们先以简单的一维梯度下降为例，解释梯度下降算法可能降低目标函数值的原因。假设连续可导的函数 f:ℝ→ℝ 的输入和输出都是标量。给定绝对值足够小的数 ϵ ，根据泰勒展开公式，我们得到以下的近似：<br><img src="http://latex.codecogs.com/gif.latex?f(x + \epsilon) \approx f(x) + \epsilon f'(x) "></p>
<p>这里 f′(x) 是函数 f 在 x 处的梯度。一维函数的梯度是一个标量，也称导数。<br>接下来，找到一个常数 η&gt;0 ，使得 <img src="http://latex.codecogs.com/gif.latex?\left|\eta f'(x)\right| ">足够小，那么可以将 ϵ 替换为 −ηf′(x) 并得到：<br><img src="http://latex.codecogs.com/gif.latex?f(x - \eta f'(x)) \approx f(x) -  \eta f'(x)^2 "></p>
<p>如果导数 f′(x)≠0 ，那么 ηf′(x)2&gt;0 ，所以<br><img src="http://latex.codecogs.com/gif.latex?f(x - \eta f'(x)) \lesssim f(x)"></p>
<p>这意味着，如果通过<br><img src="http://latex.codecogs.com/gif.latex?x \leftarrow x - \eta f'(x)"></p>
<p>来迭代 x ，函数 f(x) 的值可能会降低。因此在梯度下降中，我们先选取一个初始值 x 和常数 η&gt;0 ，然后不断通过上式来迭代 x ，直到达到停止条件，例如 <img src="http://latex.codecogs.com/gif.latex?f'(x)^2"><br>的值已足够小或迭代次数已达到某个值。</p>
<p>下面我们以目标函数<img src="http://latex.codecogs.com/gif.latex?f(x)=x^2">为例来看一看梯度下降是如何工作的。虽然我们知道最小化 f(x) 的解为 x=0 ，这里依然使用这个简单函数来观察 x 是如何被迭代的。首先，导入本节实验所需的包或模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>接下来使用 x=10 作为初始值，并设 η=0.2 。使用梯度下降对 x 迭代10次，可见最终 x 的值较接近最优解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * <span class="number">2</span> * x  <span class="comment"># f(x) = x * x的导数为f'(x) = 2 * x</span></span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>epoch 10, x: 0.06046617599999997</strong></p>
<p>下面将绘制出自变量 x 的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace</span><span class="params">(res)</span>:</span></span><br><span class="line">    n = max(abs(min(res)), abs(max(res)), <span class="number">10</span>)</span><br><span class="line">    f_line = np.arange(-n, n, <span class="number">0.1</span>)</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(f_line, [x * x <span class="keyword">for</span> x <span class="keyword">in</span> f_line])</span><br><span class="line">    d2l.plt.plot(res, [x * x <span class="keyword">for</span> x <span class="keyword">in</span> res], <span class="string">'-o'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line"></span><br><span class="line">show_trace(res)</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/mariaclark1123/mariaclark1123.github.io/blob/master/srcimage/gd1.png?raw=true" alt></p>
<h3 id="学习率η"><a href="#学习率η" class="headerlink" title="学习率η"></a>学习率η</h3><p>上述梯度下降算法中的正数 η 通常叫作学习率。这是一个超参数，需要人工设定。如果使用过小的学习率，会导致 x 更新缓慢从而需要更多的迭代才能得到较好的解。</p>
<p>下面展示使用学习率 η=0.05 时自变量 x 的迭代轨迹。可见，同样迭代10次后，当学习率过小时，最终 x 的值依然与最优解存在较大偏差。</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">0.05</span>))</span><br></pre></td></tr></table></figure>
<p><strong>epoch 10, x: 3.4867844009999995</strong><br><img src="https://github.com/mariaclark1123/mariaclark1123.github.io/blob/master/srcimage/gd2.png?raw=true" alt></p>
<p>如果使用过大的学习率， <img src="http://latex.codecogs.com/gif.latex?\left|\eta f'(x)\right| ">可能会过大从而使前面提到的一阶泰勒展开公式不再成立：这时我们无法保证迭代 x 会降低 f(x) 的值。</p>
<p>举个例子，当设学习率 η=1.1 时，可以看到 x 不断越过（overshoot）最优解 x=0 并逐渐发散。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">1.1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>epoch 10, x: 61.917364224000096</strong><br><img src="https://github.com/mariaclark1123/mariaclark1123.github.io/blob/master/srcimage/gd3.png?raw=true" alt></p>
<h3 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h3><p>在了解了一维梯度下降之后，我们再考虑一种更广义的情况：目标函数的输入为向量，输出为标量。假设目标函数 <img src="http://latex.codecogs.com/gif.latex?f: \mathbb{R}^d \rightarrow \mathbb{R}">的输入是一个 d 维向量  <img src="http://latex.codecogs.com/gif.latex?\boldsymbol{x} = [x_1, x_2, \ldots, x_d]^\top">。目标函数 f(x) 有关 x 的梯度是一个由 d 个偏导数组成的向量：</p>
<p> <img src="http://latex.codecogs.com/gif.latex?\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg]^\top."></p>
<p> 为表示简洁，我们用 ∇f(x) 代替 ∇xf(x) 。梯度中每个偏导数元素 ∂f(x)/∂xi 代表着 f 在 x 有关输入 xi 的变化率。为了测量 f 沿着单位向量 u （即 ‖u‖=1 ）方向上的变化率，在多元微积分中，我们定义 f 在 x 上沿着 u 方向的方向导数为</p>
<p>  <img src="http://latex.codecogs.com/gif.latex?\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \lim_{h \rightarrow 0}  \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}."></p>
<p>  依据方向导数性质，以上方向导数可以改写为</p>
<p> <img src="http://latex.codecogs.com/gif.latex?\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}."></p>
<p> 方向导数  <img src="http://latex.codecogs.com/gif.latex?\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})">给出了 f 在 x 上沿着所有可能方向的变化率。为了最小化 f ，我们希望找到 f 能被降低最快的方向。因此，我们可以通过单位向量 u 来最小化方向导数  <img src="http://latex.codecogs.com/gif.latex?\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})">。</p>
<p> 由于  <img src="http://latex.codecogs.com/gif.latex?\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \|\nabla f(\boldsymbol{x})\| \cdot \|\boldsymbol{u}\| \cdot \text{cos} (\theta) = \|\nabla f(\boldsymbol{x})\| \cdot \text{cos} (\theta)">， 其中 θ 为梯度 ∇f(x) 和单位向量 u 之间的夹角，当 θ=π 时， cos(θ) 取得最小值 −1 。因此，当 u 在梯度方向 ∇f(x) 的相反方向时，方向导数 Duf(x) 被最小化。因此，我们可能通过梯度下降算法来不断降低目标函数 f 的值：</p>
<p> <img src="http://latex.codecogs.com/gif.latex?\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x})."></p>
<p> 同样，其中 η （取正数）称作学习率。<br>下面我们构造一个输入为二维向量 <img src="http://latex.codecogs.com/gif.latex?\boldsymbol{x} = [x_1, x_2]^\top">和输出为标量的目标函数 <img src="http://latex.codecogs.com/gif.latex?f(\boldsymbol{x})=x_1^2+2x_2^2"> 。那么，梯度<img src="http://latex.codecogs.com/gif.latex?\nabla f(\boldsymbol{x}) = [2x_1, 4x_2]^\top">。我们将观察梯度下降从初始位置 [−5,−2] 开始对自变量 x 的迭代轨迹。我们先定义两个辅助函数，第一个函数使用给定的自变量更新函数，从初始位置 [−5,−2] 开始迭代自变量 x 共20次，第二个函数对自变量 x 的迭代轨迹进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_2d</span><span class="params">(trainer)</span>:</span>  <span class="comment"># 本函数将保存在d2lzh包中方便以后使用</span></span><br><span class="line">    x1, x2, s1, s2 = <span class="number">-5</span>, <span class="number">-2</span>, <span class="number">0</span>, <span class="number">0</span>  <span class="comment"># s1和s2是自变量状态，本章后续几节会使用</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)</span><br><span class="line">        results.append((x1, x2))</span><br><span class="line">    print(<span class="string">'epoch %d, x1 %f, x2 %f'</span> % (i + <span class="number">1</span>, x1, x2))</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace_2d</span><span class="params">(f, results)</span>:</span>  <span class="comment"># 本函数将保存在d2lzh包中方便以后使用</span></span><br><span class="line">    d2l.plt.plot(*zip(*results), <span class="string">'-o'</span>, color=<span class="string">'#ff7f0e'</span>)</span><br><span class="line">    x1, x2 = np.meshgrid(np.arange(<span class="number">-5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>), np.arange(<span class="number">-3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>))</span><br><span class="line">    d2l.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">'#1f77b4'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'x2'</span>)</span><br></pre></td></tr></table></figure>
<p>然后，观察学习率为 0.1 时自变量的迭代轨迹。使用梯度下降对自变量 x 迭代20次后，可见最终 x 的值较接近最优解 [0,0] 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<p><strong>epoch 20, x1 -0.057646, x2 -0.000073</strong><br><img src="https://github.com/mariaclark1123/mariaclark1123.github.io/blob/master/srcimage/gd4.png?raw=true" alt></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2020-04-04</span><i class="fa fa-tag"></i><a href="/tags/Machine-Learning/" title="Machine Learning" class="tag">Machine Learning </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/2020/04/04/人工智能理论基础（一） - 梯度下降（Gradient Descent）/,Fang-da,人工智能优化算法（一） - 梯度下降（Gradient Descent）,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2020/04/04/人工智能优化算法（一） - 梯度下降（Gradient Descent）/" title="人工智能优化算法（一） - 梯度下降（Gradient Descent）" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2020/04/04/人工智能数学基础（一） - 导数和偏导数/" title="人工智能数学基础（一） - 导数和偏导数" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>